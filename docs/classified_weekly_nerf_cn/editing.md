
每周分类神经辐射场 - editing ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
====================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## 大部分为机器翻译，少数论文手动翻译，有翻译错误可以PR修复。
## Jul31 - Aug6, 2022
  - [VolTeMorph：体积表示的实时、可控和可泛化动画](https://arxiv.org/pdf/2208.00949) | [***``[code]``***](https://arxiv.org/pdf/2208.00949)
    > 最近，用于场景重建和新颖视图合成的体积表示越来越受欢迎，这使人们重新关注在高可见度下对体积内容进行动画处理质量和实时性。虽然基于学习函数的隐式变形方法可以产生令人印象深刻的结果，但它们对于艺术家和内容创作者来说是“黑匣子”，它们需要大量的训练数据才能进行有意义的概括，而且它们不会在训练数据之外产生现实的外推。在这项工作中，我们通过引入一种实时、易于使用现成软件进行编辑并且可以令人信服地推断的体积变形方法来解决这些问题。为了展示我们方法的多功能性，我们将其应用于两个场景：基于物理的对象变形和远程呈现，其中化身使用混合形状进行控制。我们还进行了彻底的实验，表明我们的方法优于结合隐式变形的体积方法和基于网格变形的方法。
  - [基于神经辐射场和运动图的可控自由视点视频重建, IEEE Transactions on Visualization and Computer Graphics](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 在本文中，我们提出了一种基于运动图和神经辐射场（NeRF）的可控高质量自由视点视频生成方法。与现有的姿势驱动 NeRF 或时间/结构条件的 NeRF 工作不同，我们建议首先构建捕获序列的有向运动图。这种序列-运动-参数化策略不仅能够灵活地控制自由视点视频渲染的姿态，而且避免了相似姿态的冗余计算，从而提高了整体重建效率。此外，为了支持身体形状控制而不损失逼真的自由视点渲染性能，我们通过结合显式表面变形和隐式神经场景表示来改进 vanilla NeRF。具体来说，我们为运动图上的每个有效帧训练一个局部表面引导的 NeRF，并且体积渲染仅在真实表面周围的局部空间中执行，从而实现了合理的形状控制能力。据我们所知，我们的方法是第一个同时支持逼真的自由视点视频重建和基于运动图的用户引导运动遍历的方法。结果和比较进一步证明了所提出方法的有效性。
  - [基于神经描述符字段的鲁棒变化检测, IROS2022](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 推理环境变化的能力对于长时间运行的机器人至关重要。代理应在操作期间捕获更改，以便可以遵循操作以确保工作会话的顺利进行。然而，不同的视角和累积的定位误差使得机器人很容易由于低观察重叠和漂移的对象关联而错误地检测到周围世界的变化。在本文中，基于最近提出的类别级神经描述符字段 (NDF)，我们开发了一种对象级在线变化检测方法，该方法对部分重叠的观察和嘈杂的定位结果具有鲁棒性。利用 NDF 的形状补全能力和 SE(3) 等效性，我们表示具有紧凑形状代码的对象，该代码编码来自部分观察的完整对象形状。然后基于从 NDF 恢复的对象中心将对象组织在空间树结构中，以便快速查询对象邻域。通过形状代码相似性关联对象并比较局部对象-邻居空间布局，我们提出的方法证明了对低观测重叠和定位噪声的鲁棒性。我们对合成序列和真实世界序列进行了实验，与多种基线方法相比，实现了改进的变化检测结果。
## Jul24 - Jul30, 2022
  - [MobileNeRF：利用多边形光栅化管道在移动架构上进行高效的神经场渲染](https://arxiv.org/abs/2208.00277) | [***``[code]``***](https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf)
    > 神经辐射场 (NeRFs) 展示了从新颖视图合成 3D 场景图像的惊人能力。但是，它们依赖于基于光线行进的专用体积渲染算法，这些算法与广泛部署的 g 的功能不匹配图形硬件。本文介绍了一种基于纹理多边形的新 NeRF 表示，它可以使用标准渲染管道有效地合成新图像。 NeRF 表示为一组多边形，其纹理表示二进制不透明度和特征向量。使用 z 缓冲区对多边形进行传统渲染会生成每个像素都有特征的图像，这些图像由在片段着色器中运行的小型、依赖于视图的 MLP 进行解释，以产生最终的像素颜色。这种方法使 NeRF 能够使用传统的多边形光栅化管道进行渲染，该管道提供大规模的像素级并行性，在包括手机在内的各种计算平台上实现交互式帧速率。
  - [用笼子变形辐射场, ECCV2022](https://arxiv.org/abs/2207.12298) | [code]
    > 辐射场的最新进展可以实现静态或动态 3D 场景的逼真渲染，但仍不支持用于场景操作或动画的显式变形。在本文中，我们提出了一种新的辐射场变形方法：自由形式的辐射场变形。我们使用一个三角形网格来包围称为笼子的前景对象作为界面，通过操纵笼子顶点，我们的方法可以实现辐射场的自由变形。我们方法的核心是网格变形中常用的基于笼的变形。我们提出了一种将其扩展到辐射场的新公式，该公式将采样点的位置和视图方向从变形空间映射到规范空间，从而实现变形场景的渲染。合成数据集和真实世界数据集的变形结果证明了我们方法的有效性。
  - [NeuMesh：学习基于解缠结神经网格的隐式场，用于几何和纹理编辑, ECCV2022(oral)](https://arxiv.org/abs/2207.11911) | [code]
    > 最近，神经隐式渲染技术得到了迅速发展，并在新颖的视图合成和 3D 场景重建中显示出巨大的优势。然而，现有的用于编辑目的的神经渲染方法提供的功能有限，例如，刚性变换，或者不适用于日常生活中一般对象的细粒度编辑。在本文中，我们提出了一种新颖的基于网格的表示，通过在网格顶点上使用解开几何和纹理代码对神经隐场进行编码，这促进了一组编辑功能，包括网格引导的几何编辑、带有纹理交换的指定纹理编辑、填充和绘画操作。为此，我们开发了几种技术包括可学习的符号指标以放大基于网格的表示的空间可区分性，蒸馏和微调机制以实现稳定收敛，以及空间感知优化策略以实现精确的纹理编辑。对真实数据和合成数据的大量实验和编辑示例证明了我们的方法在表示质量和编辑能力方面的优越性。代码可在项目网页上找到：此 https URL。
## Previous weeks
  - [神经稀疏体素场, NeurIPS2020](https://lingjie0206.github.io/papers/NSVF/) | [***``[code]``***](https://github.com/facebookresearch/NSVF)
    > 我们介绍了神经稀疏体素场 (NSVF)，这是一种用于快速和高质量自由视点渲染的新神经场景表示。 NSVF 定义了一组以稀疏体素八叉树组织的体素有界隐式字段，以对每个单元中的局部属性进行建模。 我们仅从一组姿势的 RGB 图像中通过可区分的光线行进操作逐步学习底层体素结构。 使用稀疏体素八叉树结构，可以通过跳过不包含相关场景内容的体素来加速渲染新颖的视图。 我们的方法在推理时比最先进的方法（即 NeRF (Mildenhall et al., 2020)）快 10 倍以上，同时获得更高质量的结果。 此外，通过利用显式稀疏体素表示，我们的方法可以很容易地应用于场景编辑和场景合成。 我们还展示了几个具有挑战性的任务，包括多场景学习、移动人体的自由视点渲染和大规模场景渲染。
  - [CAMPARI：相机感知分解生成神经辐射场](https://arxiv.org/pdf/2103.17269.pdf) | [code]
    > 深度生成模型的巨大进步导致了逼真的图像合成。在取得令人信服的结果的同时，大多数方法都在二维图像域中运行，而忽略了我们世界的三维性质。因此，最近的几项工作提出了具有 3D 感知能力的生成模型，即场景以 3D 建模，然后可微分地渲染到图像平面。这导致了令人印象深刻的 3D 一致性，但纳入这种偏差是有代价的：相机也需要建模。当前的方法假定固定的内在函数和预先定义的相机姿势范围。因此，实际数据通常需要参数调整，如果数据分布不匹配，结果会下降。我们的关键假设是，与图像生成器一起学习相机生成器会导致更原则性的 3D 感知图像合成方法。此外，我们建议将场景分解为背景和前景模型，从而实现更有效和更清晰的场景表示。在从原始的、未定型的图像集合中进行训练时，我们学习了一个 3D 和相机感知的生成模型，它不仅忠实地恢复了图像，而且还忠实地恢复了相机数据分布。在测试时，我们的模型生成的图像可以显式控制相机以及场景的形状和外观。
  - [NeRFactor：未知光照下形状和反射率的神经分解, TOG 2021 (Proc. SIGGRAPH Asia)](https://xiuming.info/projects/nerfactor/) | [code]
    > 我们解决了从由一种未知光照条件照射的物体的多视图图像（及其相机姿势）中恢复物体的形状和空间变化反射率的问题。这使得能够在任意环境照明下渲染对象的新颖视图并编辑对象的材质属性。我们方法的关键，我们称之为神经辐射分解（NeRFactor），是提取神经辐射场（NeRF）的体积几何[Mildenhall et al。 2020] 将对象表示为表面表示，然后在解决空间变化的反射率和环境照明的同时联合细化几何。具体来说，NeRFactor 在没有任何监督的情况下恢复表面法线、光能见度、反照率和双向反射分布函数 (BRDF) 的 3D 神经场，仅使用重新渲染损失、简单的平滑先验和从真实数据中学习的数据驱动的 BRDF 先验-世界BRDF测量。通过显式建模光可见性，NeRFactor 能够从反照率中分离出阴影，并在任意光照条件下合成逼真的软阴影或硬阴影。 NeRFactor 能够恢复令人信服的 3D 模型，用于在合成场景和真实场景的这种具有挑战性且约束不足的捕获设置中进行自由视点重新照明。定性和定量实验表明，NeRFactor 在各种任务中都优于经典和基于深度学习的最新技术。我们的视频、代码和数据可在 people.csail.mit.edu/xiuming/projects/nerfactor/ 上找到。
  - [以对象为中心的神经场景渲染](https://shellguo.com/osf/) | [***``[code]``***](https://shellguo.com/osf/)
    > 我们提出了一种从捕获的对象图像中合成逼真场景的方法。我们的工作建立在神经辐射场 (NeRFs) 之上，它隐含地模拟了场景的体积密度和定向发射的辐射。虽然 NeRF 可以合成逼真的图片，但它们只对静态场景进行建模，并且与特定的成像条件密切相关。这个属性使得 NeRFs 难以泛化到新场景，包括新的光照或对象的新排列。我们建议学习以对象为中心的神经散射函数 (OSF)，而不是像 NeRF 那样学习场景辐射场，这是一种使用与光照和视图相关的神经网络隐式模拟每个对象的光传输的表示。即使物体或灯光移动，这也可以渲染场景，而无需重新训练。结合体积路径跟踪程序，我们的框架能够渲染对象内和对象间的光传输效果，包括遮挡、镜面反射、阴影和间接照明。我们评估了我们的场景合成方法，并表明它可以推广到新的照明条件，产生逼真的、物理上精确的多对象场景渲染。
  - [物体辐射场的无监督发现, ICLR2022](https://arxiv.org/abs/2107.07905) | [code]
    > 我们研究从单个图像推断以对象为中心的场景表示的问题，旨在推导出解释图像形成过程的表示，捕捉场景的 3D 性质，并且在没有监督的情况下学习。由于将复杂的 3D 到 2D 图像形成过程集成到强大的推理方案（如深度网络）中存在根本性挑战，大多数现有的场景分解方法都缺乏这些特征中的一个或多个。在本文中，我们提出了对象辐射场 (uORF) 的无监督发现，将神经 3D 场景表示和渲染的最新进展与深度推理网络相结合，用于无监督 3D 场景分解。在没有注释的多视图 RGB 图像上进行训练，uORF 学习从单个图像分解具有不同纹理背景的复杂场景。我们展示了 uORF 在无监督 3D 场景分割、新视图合成和三个数据集上的场景编辑方面表现良好。
  - [学习用于可编辑场景渲染的对象组合神经辐射场, ICCV2021](https://zju3dv.github.io/object_nerf/) | [***``[code]``***](https://github.com/zju3dv/object_nerf)
    > 隐式神经渲染技术已经显示出用于新视图合成的有希望的结果。然而，现有方法通常将整个场景编码为一个整体，这通常不知道对象身份，并且限制了移动或添加家具等高级编辑任务的能力。在本文中，我们提出了一种新颖的神经场景渲染系统，该系统学习对象组成的神经辐射场，并为集群和真实世界场景生成具有编辑能力的逼真渲染。具体来说，我们设计了一种新颖的双路径架构，其中场景分支对场景几何和外观进行编码，对象分支根据可学习的对象激活码对每个独立对象进行编码。为了在严重混乱的场景中进行训练，我们提出了一种场景引导的训练策略来解决遮挡区域中的 3D 空间模糊性并学习每个对象的清晰边界。大量实验表明，我们的系统不仅在静态场景新视图合成方面取得了有竞争力的性能，而且为对象级编辑产生了逼真的渲染。
  - [编辑条件辐射场, ICCV2021](http://editnerf.csail.mit.edu/) | [***``[code]``***](https://github.com/stevliu/editnerf)
    > 神经辐射场 (NeRF) 是支持高质量视图合成的场景模型，针对每个场景进行了优化。在本文中，我们探索启用用户编辑类别级 NeRF - 也称为条件辐射场 - 在形状类别上训练。具体来说，我们介绍了一种将粗略的 2D 用户涂鸦传播到 3D 空间的方法，以修改局部区域的颜色或形状。首先，我们提出了一个条件辐射场，它结合了新的模块化网络组件，包括一个跨对象实例共享的形状分支。观察同一类别的多个实例，我们的模型在没有任何监督的情况下学习底层部分语义，从而允许将粗略的 2D 用户涂鸦传播到整个 3D 区域（例如，椅子座位）。接下来，我们提出了一种针对特定网络组件的混合网络更新策略，该策略平衡了效率和准确性。在用户交互过程中，我们制定了一个既满足用户约束又保留原始对象结构的优化问题。我们在三个形状数据集上展示了我们在各种编辑任务上的方法，并表明它优于以前的神经编辑方法。最后，我们编辑真实照片的外观和形状，并显示编辑传播到外推的新视图。
  - [使用分层神经表示的可编辑自由视点视频, SIGGRAPH2021](https://jiakai-zhang.github.io/st-nerf/) | [***``[code]``***](https://jiakai-zhang.github.io/st-nerf/#code)
    > 生成自由视点视频对于沉浸式 VR/AR 体验至关重要，但最近的神经学进展仍然缺乏编辑能力来操纵大型动态场景的视觉感知。为了填补这一空白，在本文中，我们提出了第一种仅使用稀疏的 16 个摄像头为大规模动态场景生成可编辑照片般逼真的自由视点视频的方法。我们方法的核心是一种新的分层神经表示，其中包括环境本身的每个动态实体都被制定为称为 ST-NeRF 的时空相干神经分层辐射表示。这种分层表示支持对动态场景的完全感知和真实操作，同时仍支持大范围的自由观看体验。在我们的 ST-NeRF 中，动态实体/层被表示为连续函数，以连续和自监督的方式实现动态实体的位置、变形以及外观的解耦。我们提出了一个场景解析 4D 标签映射跟踪来显式地解开空间信息，以及一个连续变形模块来隐式地解开时间运动。进一步引入了一种对象感知体绘制方案，用于重新组装所有神经层。我们采用了一种新颖的分层损失和运动感知光线采样策略，以实现对具有多个表演者的大型动态场景的有效训练，我们的框架进一步实现了各种编辑功能，即操纵规模和位置，复制或重新定时单个神经层在保持高度真实感的同时创造众多视觉效果。大量实验证明了我们的方法在为动态场景生成高质量、照片般逼真和可编辑的自由视点视频方面的有效性。
