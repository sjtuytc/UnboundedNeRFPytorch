
每周分类神经辐射场 - reconstruction ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
===========================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## 大部分为机器翻译，少数论文手动翻译，有翻译错误可以PR修复。
## Jul31 - Aug6, 2022
  - [PRIF: Primary Ray-based Implicit Function](https://research.google/pubs/pub51556/) | [code]
    > 我们引入了一种新的隐式形状表示，称为基于初级光线的隐式函数 (PRIF)。与大多数基于符号距离函数 (SDF) 处理空间位置的现有方法相比，我们的表示在定向射线上运行。具体来说，PRIF 被制定为直接生成给定输入射线的表面命中点，而无需昂贵的球体跟踪操作，从而实现高效的形状提取和可微渲染。我们证明了经过训练以编码 PRIF 的神经网络在各种任务中取得了成功，包括单一形状表示、类别形状生成、稀疏或嘈杂观察的形状补全、相机姿态估计的逆渲染以及颜色的神经渲染。
## Jul24 - Jul30, 2022
  - [脱离网格：用于 3D 血管建模的连续隐式神经表示, MICCAI STACOM 2022](https://arxiv.org/abs/2207.14663) | [code]
    > 个性化 3D 血管模型对于心血管疾病患者的诊断、预后和治疗计划非常有价值。传统上，此类模型是用网格和体素掩码等显式表示或径向基函数或原子（管状）形状等隐式表示构建的。在这里，我们建议在可微的隐式神经表示 (INR) 中通过其有符号距离函数 (SDF) 的零水平集来表示表面。这使我们能够用隐式、连续、轻量级且易于与深度学习算法集成的表示来对复杂的血管结构进行建模。我们在这里通过三个实际示例展示了这种方法的潜力。首先，我们从 CT 图像中获得了腹主动脉瘤 (AAA) 的准确且防水的表面，并从表面上的 200 个点显示出稳健的拟合。其次，我们同时将嵌套的血管壁安装在单个 INR 中，没有交叉点。第三，我们展示了如何将单个动脉的 3D 模型平滑地融合到单个防水表面中。我们的结果表明，INR 是一种灵活的表示形式，具有最小交互注释的潜力复杂血管结构的研究和操作。
  - [GAUDI：沉浸式 3D 场景生成的神经架构师](https://arxiv.org/abs/2207.13751) | [***``[code]``***](https://github.com/apple/ml-gaudi)
    > 我们介绍了 GAUDI，这是一种生成模型，能够捕捉复杂而逼真的 3D 场景的分布，可以从移动的相机中沉浸式地渲染。我们用一种可扩展但功能强大的方法来解决这个具有挑战性的问题，我们首先优化一个潜在的表示，以解开辐射场和相机姿势。然后使用这种潜在表示来学习生成模型，该模型可以无条件和有条件地生成 3D 场景.我们的模型通过消除相机姿态分布可以跨样本共享的假设来概括以前专注于单个对象的工作。我们展示了 GAUDI 在跨多个数据集的无条件生成设置中获得了最先进的性能，并允许在给定条件变量（如稀疏图像观察或描述场景的文本）的情况下有条件地生成 3D 场景。
  - [AlignSDF：用于手对象重建的姿势对齐有符号距离场, ECCV2022](https://arxiv.org/abs/2207.12909) | [***``[code]``***](https://zerchen.github.io/projects/alignsdf.html)
    > 最近的工作在从单目彩色图像联合重建手和操纵对象方面取得了令人瞩目的进展。现有方法侧重于参数网格或符号距离场 (SDF) 方面的两种替代表示。一方面，参数模型可以从先验知识中受益，但代价是有限的形状变形和网格分辨率。因此，网格模型可能无法精确重建细节，例如手和物体的接触面。另一方面，基于 SDF 的方法可以表示任意细节，但缺乏明确的先验。在这项工作中，我们的目标是使用参数表示提供的先验改进 SDF 模型。特别是，我们提出了一个联合学习框架，可以解开姿势和形状。我们从参数模型中获取手和物体的姿势，并使用它们在 3D 空间中对齐 SDF。我们表明，这种对齐的 SDF 更好地专注于重建形状细节并提高手和物体的重建精度。我们评估了我们的方法，并在具有挑战性的 ObMan 和 DexYCB 基准上展示了对现有技术的显着改进。
  - [NeuMesh：学习基于解缠结神经网格的隐式场，用于几何和纹理编辑, ECCV2022(oral)](https://arxiv.org/abs/2207.11911) | [code]
    > 最近，神经隐式渲染技术得到了迅速发展，并在新颖的视图合成和 3D 场景重建中显示出巨大的优势。然而，现有的用于编辑目的的神经渲染方法提供的功能有限，例如，刚性变换，或者不适用于日常生活中一般对象的细粒度编辑。在本文中，我们提出了一种新颖的基于网格的表示，通过在网格顶点上使用解开几何和纹理代码对神经隐场进行编码，这促进了一组编辑功能，包括网格引导的几何编辑、带有纹理交换的指定纹理编辑、填充和绘画操作。为此，我们开发了几种技术包括可学习的符号指标以放大基于网格的表示的空间可区分性，蒸馏和微调机制以实现稳定收敛，以及空间感知优化策略以实现精确的纹理编辑。对真实数据和合成数据的大量实验和编辑示例证明了我们的方法在表示质量和编辑能力方面的优越性。代码可在项目网页上找到：此 https URL。
## Previous weeks
  - [非刚性神经辐射场：单目视频变形场景的重建和新视图合成，, ICCV2021](https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/) | [***``[code]``***](https://github.com/facebookresearch/nonrigid_nerf)
    > 我们提出了非刚性神经辐射场 (NR-NeRF)，这是一种用于一般非刚性动态场景的重建和新颖的视图合成方法。我们的方法将动态场景的 RGB 图像作为输入（例如，来自单目视频记录），并创建高质量的时空几何和外观表示。我们表明，单个手持消费级相机足以从新颖的虚拟相机视图合成动态场景的复杂渲染，例如一个“子弹时间”的视频效果。 NR-NeRF 将动态场景分解为规范体积及其变形。场景变形被实现为光线弯曲，其中直线光线被非刚性变形。我们还提出了一种新的刚性网络来更好地约束场景的刚性区域，从而获得更稳定的结果。射线弯曲和刚性网络在没有明确监督的情况下进行训练。我们的公式可以实现跨视图和时间的密集对应估计，以及引人注目的视频编辑应用程序，例如运动夸张。我们的代码将是开源的。
  - [神经关节辐射场, ICCV2021](https://arxiv.org/abs/2104.03110) | [***``[code]``***](https://github.com/nogu-atsu/NARF#code)
    > 我们提出了神经关节辐射场 (NARF)，这是一种新颖的可变形 3D 表示，用于从图像中学习到的关节对象。虽然 3D 隐式表示的最新进展使得学习复杂对象的模型成为可能，但学习关节对象的姿势可控表示仍然是一个挑战，因为当前的方法需要 3D 形状监督并且无法呈现外观。在制定 3D 关节对象的隐式表示时，我们的方法在求解每个 3D 位置的辐射场时仅考虑最相关对象部分的刚性变换。通过这种方式，所提出的方法可以表示与姿势相关的变化，而不会显着增加计算复杂度。 NARF 是完全可微的，可以从带有姿势注释的图像中训练出来。此外，通过使用自动编码器，它可以学习对象类的多个实例的外观变化。实验表明，所提出的方法是有效的，并且可以很好地推广到新的姿势。
  - [GRF：学习用于 3D 场景表示和渲染的一般辐射场, ICCV2021(oral)](https://arxiv.org/abs/2010.04595) | [***``[code]``***](https://github.com/alextrevithick/GRF)
    > 我们提出了一个简单而强大的神经网络，它仅从 2D 观察中隐式表示和渲染 3D 对象和场景。该网络将 3D 几何建模为一般辐射场，它以一组具有相机位姿和内在函数的 2D 图像作为输入，为 3D 空间的每个点构建内部表示，然后渲染该点的相应外观和几何观察从任意位置。我们方法的关键是学习 2D 图像中每个像素的局部特征，然后将这些特征投影到 3D 点，从而产生一般和丰富的点表示。我们还集成了一种注意力机制来聚合来自多个 2D 视图的像素特征，从而隐式考虑视觉遮挡。大量实验表明，我们的方法可以为新物体、看不见的类别和具有挑战性的现实世界场景生成高质量和逼真的新视图。
  - [MVSNeRF：从多视图立体快速概括辐射场重建, ICCV2021](https://apchenstu.github.io/mvsnerf/) | [***``[code]``***](https://github.com/apchenstu/mvsnerf)
    > 我们提出了 MVSNeRF，一种新颖的神经渲染方法，可以有效地重建神经辐射场以进行视图合成。与先前的神经辐射场工作考虑对密集捕获的图像进行逐场景优化不同，我们提出了一个通用的深度神经网络，它可以通过快速网络推理仅从三个附近的输入视图重建辐射场。我们的方法利用平面扫描成本体积（广泛用于多视图立体）进行几何感知场景推理，并将其与基于物理的体积渲染相结合用于神经辐射场重建。我们在 DTU 数据集中的真实对象上训练我们的网络，并在三个不同的数据集上对其进行测试，以评估其有效性和普遍性。我们的方法可以跨场景（甚至是室内场景，与我们的对象训练场景完全不同）进行泛化，并仅使用三个输入图像生成逼真的视图合成结果，显着优于可泛化辐射场重建的并行工作。此外，如果捕捉到密集的图像，我们估计的辐射场表示可以很容易地进行微调；与 NeRF 相比，这导致具有更高渲染质量和更短优化时间的快速每场景重建。
  - [使用 NeRF 实现新视图合成的连续深度 MPI, ICCV2021](https://arxiv.org/abs/2103.14910) | [***``[code]``***](https://github.com/vincentfung13/MINE)
    > 在本文中，我们建议 MINE 通过从单个图像进行密集 3D 重建来执行新颖的视图合成和深度估计。我们的方法是通过引入神经辐射场 (NeRF) 对多平面图像 (MPI) 进行连续深度泛化。给定单个图像作为输入，MINE 预测任意深度值的 4 通道图像（RGB 和体积密度）以联合重建相机平截头体并填充被遮挡的内容。然后可以使用可微分渲染轻松地将重建和修复的截锥体渲染为新颖的 RGB 或深度视图。在 RealEstate10K、KITTI 和 Flowers Light Fields 上进行的大量实验表明，我们的 MINE 在新颖的视图合成中大大优于最先进的技术。我们还在 iBims-1 和 NYU-v2 的深度估计方面取得了具有竞争力的结果，而无需注释深度监督。我们的源代码可在此 https 网址获得
  - [UNISURF：统一神经隐式表面和辐射场以进行多视图重建, ICCV2021(oral)](https://arxiv.org/abs/2104.10078) | [***``[code]``***](https://github.com/autonomousvision/unisurf)
    > 神经隐式 3D 表示已成为从多视图图像重建表面和合成新视图的强大范例。不幸的是，DVR 或 IDR 等现有方法需要精确的每像素对象掩码作为监督。同时，神经辐射场已经彻底改变了新的视图合成。然而，NeRF 的估计体积密度不允许精确的表面重建。我们的主要见解是隐式表面模型和辐射场可以以统一的方式制定，从而使用相同的模型实现表面和体积渲染。这种统一的视角实现了新颖、更有效的采样程序，并能够在没有输入掩码的情况下重建准确的表面。我们在 DTU、BlendedMVS 和合成室内数据集上比较我们的方法。我们的实验表明，我们在重建质量方面优于 NeRF，同时在不需要掩码的情况下与 IDR 相当。
  - [NeuS：通过体渲染学习神经隐式表面以进行多视图重建, NeurIPS2021](https://arxiv.org/abs/2106.10689) | [***``[code]``***](https://github.com/Totoro97/NeuS)
    > 我们提出了一种新的神经表面重建方法，称为 NeuS，用于从 2D 图像输入中重建具有高保真度的对象和场景。现有的神经表面重建方法，如 DVR 和 IDR，需要前景掩码作为监督，容易陷入局部最小值，因此难以重建具有严重自遮挡或薄结构的物体。同时，最近用于新视图合成的神经方法，例如 NeRF 及其变体，使用体积渲染来生成具有优化鲁棒性的神经场景表示，即使对于高度复杂的对象也是如此。然而，从这种学习到的隐式表示中提取高质量的表面是很困难的，因为表示中没有足够的表面约束。在 NeuS 中，我们建议将表面表示为有符号距离函数 (SDF) 的零级集，并开发一种新的体绘制方法来训练神经 SDF 表示。我们观察到传统的体绘制方法会导致表面重建的固有几何误差（即偏差），因此提出了一种新的公式，该公式在一阶近似中没有偏差，从而即使没有掩模监督也能实现更准确的表面重建.在 DTU 数据集和 BlendedMVS 数据集上的实验表明，NeuS 在高质量表面重建方面优于最先进的技术，特别是对于具有复杂结构和自遮挡的物体和场景。
  - [神经隐式表面的体积渲染, NeurIPS2021](https://arxiv.org/abs/2106.12052) | [code]
    > 神经体绘制最近变得越来越流行，因为它成功地从一组稀疏的输入图像中合成了场景的新视图。到目前为止，通过神经体绘制技术学习的几何图形是使用通用密度函数建模的。此外，几何本身是使用密度函数的任意水平集提取的，这会导致嘈杂的、通常是低保真度的重建。本文的目标是改进神经体绘制中的几何表示和重建。我们通过将体积密度建模为几何形状的函数来实现这一点。这与之前将几何建模为体积密度函数的工作形成对比。更详细地说，我们将体积密度函数定义为应用于有符号距离函数 (SDF) 表示的拉普拉斯累积分布函数 (CDF)。这种简单的密度表示具有三个好处：（i）它为在神经体绘制过程中学习的几何图形提供了有用的归纳偏差； (ii) 它有助于限制不透明度近似误差，从而实现对视线的准确采样。准确的采样对于提供几何和辐射的精确耦合很重要； (iii) 它允许在体积渲染中对形状和外观进行有效的无监督解开。将这种新的密度表示应用于具有挑战性的场景多视图数据集产生了高质量的几何重建，优于相关的基线。此外，由于两者的分离，可以在场景之间切换形状和外观。
